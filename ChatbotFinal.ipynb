{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "BelaTheChatbotFinal.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGcodG4DCdJd"
      },
      "source": [
        "### Import Libraries for NLP and TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9S-UvPBCdJf"
      },
      "source": [
        "#important libraries\n",
        "import numpy as np # provides fast mathematical function processing\n",
        "import tensorflow as tf # machine learning framework\n",
        "from tensorflow.keras.models import Sequential # for plain layers where each layer has exactly one input tensor and one output tensor\n",
        "from tensorflow.keras.layers import Dense, Dropout # regular densely-connected neural network layer, applies dropout to the input\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer # vectorize text into integers\n",
        "import random # generate random numbers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9wHDRAlCdJm"
      },
      "source": [
        "### Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8bu7vBECdJp"
      },
      "source": [
        "#load chatbot intents \n",
        "\n",
        "import json\n",
        "with open('Chatbot_Intents.json') as file:\n",
        "  data=json.load(file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uThXkv_yCdJx"
      },
      "source": [
        "### Text Pre-Processing with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J3KrnJ5CdJy"
      },
      "source": [
        "# Initiate stemming object\n",
        "# NLP:for example -- \"roaster\", \"roasting\", \"roasts\" ---> \"roast\"\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peTLp1DbCdJ2"
      },
      "source": [
        "#run for first time\n",
        "nltk.download('punkt')\n",
        "\n",
        "# consist of unique stemmed words/tokens from patterns extended in this list. No duplicates\n",
        "words = []\n",
        "# consist of tag words from intent\n",
        "labels = []\n",
        "# consist of tokenized sentences from patterns appended in this list\n",
        "doc_x = []\n",
        "# consists of tag words from intent matching tokens in doc_x\n",
        "doc_y = []\n",
        "\n",
        "# loop through each sentences in the data/intent\n",
        "for intent in data['intents']:\n",
        "    # loop through each sentences in patterns in intent\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each words in the pattern in intent\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        # method iterates over its argument adding each element to the list by extending the list\n",
        "        words.extend(wrds)\n",
        "        # method adds its argument as a single element to the end of a list. Length of the list increase by one\n",
        "        doc_x.append(wrds)\n",
        "        doc_y.append(intent['tag'])\n",
        "        \n",
        "    if intent['tag'] not in labels:\n",
        "        labels.append(intent['tag'])\n",
        "\n",
        "# stems and lower case the words \n",
        "words = [stemmer.stem(w.lower()) for w in words if w != '?']\n",
        " \n",
        "# set() removes duplicates, list() change into a list and sorted() sort in ascending order\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "labels = sorted(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FH5iSP4XCdJ7"
      },
      "source": [
        "# consist of tokenized sentences from patterns appended in this list\n",
        "print (len(doc_x), 'documents x --->', doc_x[:20]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mxv_DCg1CdJ-"
      },
      "source": [
        "# consists of tag words from intent matching tokens in doc_x\n",
        "print (len(doc_y), 'documents y --->', doc_y[:20]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOM73OqeCdKC"
      },
      "source": [
        "# consist of tag words from intent\n",
        "print (len(labels), 'labels --->', labels[:20]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq7mWlIWCdKE"
      },
      "source": [
        "# consist of unique stemmed words/tokens from patterns\n",
        "print (len(words), 'unique stemmed words', words[:30]) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3vAT0FaCdKG"
      },
      "source": [
        "### Transformation of Text in the Corpus to Vector of Numbers as Input to ML Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hH1jJA6kCdKH"
      },
      "source": [
        "# creating training data from corpus. Change texts into array of numbers\n",
        "# Bag of words (Bow) is a method to extract features from text documents. These features can be used to train ML model. \n",
        "# Bow creates a vocabulary of all the unique words in documents in the training set\n",
        "# Bow disregards order in which they appear\n",
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "# empty array for output\n",
        "out_empty = [0 for _ in range(len(labels))]\n",
        "\n",
        "# create bag of words for each sentences \n",
        "for x, doc in enumerate(doc_x):\n",
        "    # initialize bag of words\n",
        "    bag = []\n",
        "    # stem and change all words to lower case\n",
        "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
        "    # use for loop to create an array of bag of words\n",
        "    for w in words:\n",
        "        bag.append(1) if w in wrds else bag.append(0)\n",
        "\n",
        "    # output is a '0' for each tag and '1' for current tag\n",
        "    output_row = out_empty[:]\n",
        "    output_row[labels.index(doc_y[x])] = 1\n",
        "\n",
        "    # result of 'bag' added to training list\n",
        "    X_train.append(bag)\n",
        "    # result of 'output_row' added to output list\n",
        "    y_train.append(output_row)\n",
        "\n",
        "# change to numpy array\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSEy2dVeCdKI"
      },
      "source": [
        "# shows sparse vector (lots of sero values) due to big documents \n",
        "print(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nlr-XqAYCdKK"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5F4GhncCdKM"
      },
      "source": [
        "### Create Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1ngpXzZCdKO"
      },
      "source": [
        "# build model architecture\n",
        "# dense 128 ---> unit or number of neurons\n",
        "# droupout layers with rate of 0.5 are added to \"turn off\" neurons during training to prevent overfitting\n",
        "# The length of teh vector = vocabulary size (how many unique words in the document without duplicates)\n",
        "# categorical crossentropy loss function is used in multi-class classification tasks \n",
        "\n",
        "model=Sequential()\n",
        "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(y_train[0]), activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-xo0R9vCdKP"
      },
      "source": [
        "# summarize the architecture of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuxhCtgOCdKR"
      },
      "source": [
        "# train model\n",
        "model.fit(X_train, y_train, epochs=700, batch_size=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEo16Q7yCdKS"
      },
      "source": [
        "test_loss, test_acc = model.evaluate(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2osQpGfpCdKV"
      },
      "source": [
        "### Transformation of User Input Text to Vector of Number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDnsos92CdKV"
      },
      "source": [
        "# function to change user input into array of numbers \n",
        "\n",
        "def bag_of_words(s, words):\n",
        "    bag = [0 for _ in range(len(words))]\n",
        "\n",
        "    s_words = nltk.word_tokenize(s)\n",
        "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
        "\n",
        "    for se in s_words:\n",
        "        # adds a counter to an iterable and returns it in a form of numbered object\n",
        "        for i, w in enumerate(words):\n",
        "            if w == se:\n",
        "                bag[i] = 1\n",
        "            \n",
        "    return np.array(bag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7L67w3AeCdKW"
      },
      "source": [
        "### ChatBot Response"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAdX5PooCdKX"
      },
      "source": [
        "# function to allow user input and chatbot response\n",
        "\n",
        "def start_chat():\n",
        "    # first sentence to initiate communication between bot and user\n",
        "    print('Hello, My name is Bela the Robot. I will answer your questions about Technical Analysis and Financial Data Science. Input a Financial related word and I will help you! If you want to exit, type Bye')\n",
        "    \n",
        "    while True:\n",
        "        # prompt user to respond\n",
        "        user_input = input('User: ')\n",
        "        # exit word to terminate the while loop\n",
        "        if user_input.lower() == \"bye\":\n",
        "            break\n",
        "              \n",
        "        # predict the correct label given user input and comparing it to the words in pattern of intent \n",
        "        results = model.predict(bag_of_words(user_input, words).reshape(-1,436))\n",
        "        # returns the indices of the maximum values along an axis\n",
        "        results_index = np.argmax(results)\n",
        "        # return the label(tag) that best match the user input   \n",
        "        user_tag = labels[results_index]\n",
        "        #print(results.max()) # -- shows the highest probability for each chosen tag\n",
        "\n",
        "        \n",
        "        # condition set - only result with probability more than 0.9 will be considered correct respond\n",
        "        if results.max() > 0.9:\n",
        "        # prints out the responses form matching tag randomly\n",
        "            for tag_selection in data['intents']:\n",
        "                if tag_selection['tag'] == user_tag:\n",
        "                    responses = tag_selection['responses']\n",
        "            print(random.choice(responses))\n",
        "        \n",
        "        # user input with probability < 0.9, will get this message\n",
        "        else:\n",
        "            print(\"Sorry I didn't get that. Please try again or go to https://worlddatascience.tech/datapedia for more assistance\")\n",
        "         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXBuOkUxCdKZ",
        "outputId": "163d071e-64df-4de2-fa5d-7e054305e75d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "start_chat()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello, My name is Bela the Robot. I will answer your questions about Technical Analysis and Financial Data Science. Input a Financial related word and I will help you! If you want to exit, type Bye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYJRWA4cK-WF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}